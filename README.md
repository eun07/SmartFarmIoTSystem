# Build Data Pipeline of Smart Farm IoT System with monitoring 

# Overview
## Scenario
>Collect lots of data from sensor that installed on the IoT Device, based on these data send streaming data to log storage through data pipeline. These data can be monitored and visualized based on timeline.
## Requirements
1. Farmer can monitor real time data from smart farm sensor. <br>
2. The logs that generated from application should be delivered on real-time through data pipeline on the log storage.<br>
3. Logs can be visualized based on time-series.<br>
4. All of the services should be formed by serverless resource.
## Resource
### AWS
- API Gateway, Lambda, Kinesis Data Stream, Kinesis Firehose, Open Search Service, S3, Open Search dashboard
### IaC
- Terraform
## Architecture
![image](https://user-images.githubusercontent.com/55527712/183580369-8380c9fa-651c-4b16-ac1e-3e759c647074.png)

### Step 1: Data Production
***âœ” API Gateway to Lambda***

When **Lambda** is triggered by **API Gateway** , **Lambda** put the data on the **Kinesis Data Stream** record that collected from sensor.
 

### Step 2: Data Pipeline
***âœ” Kiensis Data Stream to Kiensis Firehose***

The data that is streaming on the **Kinesis Data Stream** send the streaming data to **Kinesis Firehose**. 
The data on the **Kinesis Firehose** arrived at the **Open Search Service** and **S3** for back up.

### Step 3: Storing on S3 to lambda

***âœ” S3 to Lambda***
 
 When the data are stored at **S3** and if the speicific data value excceds the threshold,
  **Lambda** would send the discord webhook alarm.


![Image](https://user-images.githubusercontent.com/55527712/183611090-fd18ea7b-59b3-4716-a7eb-c150f2f465f6.png)

### Step 4: Visualization
***âœ” Firehose to Open Search Service & Open Search Dashboard***

Since the data is arrived at open search service from **Kinesis Firehose**, the new document is created on the open search index,
 which is mapping based on data field type. The mapping data on the **Open Search Service** can be visualized on the 
 **Open Search Dashboard**.

<br>

<img width="696" alt="image" src="https://cdn.discordapp.com/attachments/779649485680279552/1020238517344944178/2022-08-11_1.51.26.png">

## Install Requirements
- AWS Root Account 
- AWS CLI Install and AWS Credintial Configure setting
- Git Install
- Terraform Install

## Deployment
1. Make and move the new directory. Git Clone Github Repository on the directory.
    ```
    git clone https://github.com/cs-devops-bootcamp/devops-02-Final-TeamD-scenario1.git
    ```
2. Change Directory to terraform Dierctory.
    ```
    cd terraform
    ```
3. On the Command line, run it thorugh with a command such as :
    ```
    terraform init
    ```

4. On the command line, deploy on the AWS with a command such as :
   <br>
  It will take about 15 minutes to deploy on the AWS.
    ```
    terraform apply
    ```
5. When the Deployment is completed, go to the Amazon Open Search Service on the AWS Management Console.<br> ID: admin PW: #Devops02

6. Enter the OpenSearch Dash board URL with deployed domain.

7. Make a role for user accessing on the open search cluster and index.
![role](https://user-images.githubusercontent.com/78151046/184048274-9f26076b-eedb-4afe-8376-2c7e23b0b6da.jpg)

    - Create Role 
    
    - Make a Name(whatever) 
    
    - Cluster permissions - `cluster_all`, `indices_all` assign 
    
    - Index permissions - index insert `*` , on the Index permissions assign same roles with Cluster permissons
    - Ignore the other stuff
1. When the role is created, role should be mapping on the user.
![mapping](https://user-images.githubusercontent.com/78151046/184048300-30622a82-343b-481c-b2f7-8bcd390bea64.jpg)
    - Select created Role name -> Select Mapped users -> Select Manage mapping
    - Insert the AWS own user ARN on the Users. (Also, insert the IAM user ARN on the Users) 
    - For the accessing the open serach service, make the new role and assign `AmazonOpenSearchFullAccess` policy on the AWS IAM.
    - Insert the previous created role on the Backend roles.

## Indexing
Move the OpenSearch Dashboard to Dev tool. Copy and Paste the below code and exeucte. This mapping process can enable to change `geo_point` proper data type. 

```
PUT /weather
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  },
  
  "mappings": {
      "properties" : {
        "co2" : {
          "type" : "long"
        },
        "coord" : {
          "type": "geo_point"
        },
        "device_id" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "error_code" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "humidity" : {
          "type" : "long"
        },
        "pressure" : {
          "type" : "long"
        },
        "result" : {
          "type" : "text",
          "fields" : {
            "keyword" : {
              "type" : "keyword",
              "ignore_above" : 256
            }
          }
        },
        "server_time" : {
          "type" : "date"
        },
        "temperature" : {
          "type" : "long"
        }
      }
    }
}
```
If the process is over, enter the OpenSearch Dashboards - Management - Stack Management - Index patterns.

Select the speific index to create index pattern and make index.

Check the proper data type on the index pattern.

## How it works
>Send the GET Request on the API Gateway Endpoint, lambda is triggered and send log data to Kinesis Data Stream. 
Kinesis Data Stream deliver to the Kinesis Firehose, which push the data on open search service and s3 for back  up. When the data is stored at S3, lambda is called. The speific data value exceeds the threshold, lambda function will automatically send alarm message on the discord. Open Search Service can consume data from Kinesis Firehose and visualize on the Open Search Dashboard.

## Testing
![test](https://user-images.githubusercontent.com/78151046/184051204-38895242-8d08-4f33-8cdd-56a3d4f5bdc8.jpg)

When the API Gateway got the GET Requst from endpoint, sensor data can be viewed on the Open Search Dashboard after 1~2 minutes later.

## Clean up
Delete the data on the S3 Bucket.

On the terraform directory exeucte command such as : <br>
It will take about 10 mintues to clean up resource.
```
terraform destroy
```



## Architecture Resource 
**ğŸ’¡ Real-time**
>The sensor information installed in the smart farm sensor is frequently viewed, and it need to handle the data on real time. To meet these requirements, we chose **kinesis data stream** and **kinesis firehose**, which can handle real time data and manage streaming. Real-time log data was delivered streaming format to **kinesis data stream**. Configuring efficient of data pipeline we have designed **kinesis data stream** to **kinesis firehose**, which can be attached with other resources.

**ğŸ’¡ Timestamp**
>Smart farm data from sensor should be viewed time-series for client. We used the open search to classify data based on timeline. When the data is directly delivered to open search from **kinesis firehose**, the classified data is shown based on data types with time.

**ğŸ’¡ Real-time**
>We chose the **open search dashboard** to provide client monitoring service. When the new document is created in open search index, which is indexing with data classification. The data is classified based on metrics and the classified results are viewed on the open search dashboard.<br>
On the **open search dashboard**, we can customize the dashboard depending on needs of client. Basically, our **open search dashboard** provides client with data visualization that indicates smart farmâ€™s status. According to the client requirements, we would query easily from **open search service** and visualize specific metric.<br>
When the **kinesis firehose** send data to **S3** back up, the **lambda** function is called to analysis the log data. If the specific sensor data value exceeded the standard then **lambda** function send to discord alarm immediately. For end client, thatâ€™s always critical to be alerted immediately.
<!-- > **AWS Lambda**
<br>LambdaëŠ” ì„œë²„ë¦¬ìŠ¤ ì„œë¹„ìŠ¤ë¡œ ì„œë²„ì— ëŒ€í•œ Managed Serviceë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ë§ ê·¸ëŒ€ë¡œ ì„œë²„ë¥¼ ì‹ ê²½ì“°ì§€ ì•Šì•„ë„ ë˜ê¸° ë•Œë¬¸ì— ì„œë²„ë¦¬ìŠ¤ë¼ê³  ë¶€ë¥¸ë‹¤.Lambdaë¥¼ í†µí•´ì„œ ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì„±í•¨ì— ìˆì–´ì–´ ë¹„ìš©ê³¼ ì„œë²„ì— ëŒ€í•œ ê´€ë¦¬ë¥¼ ì¤„ì´ê³ ì ì‘ì€ ë‹¨ìœ„ì˜ í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” Lambdaë¥¼ ì„ íƒí•´ì„œ ìš”ì²­ì´ ë“¤ì–´ì˜¬ë•Œë§ˆë‹¤ ì²˜ë¦¬í•´ì£¼ë„ë¡ êµ¬í˜„í–ˆë‹¤. 
  
> **AWS Kiensis Data Stream, Kinesis Firehose**
> <br>KinesisëŠ” ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ë‹¤ë£¨ëŠ”ë° ìœ ìš©í•˜ê³  ë³´ë‹¤ ì•ˆì •ì ì¸ IoT ë””ë°”ì´ìŠ¤ì—ì„œ ì˜¤ëŠ” ì •ë³´ì˜ ê²½ìš° ì‹¤ì‹œê°„ìœ¼ë¡œ ì •ë³´ê°€ ê³„ì† ì „ë‹¬ë˜ê¸° ë•Œë¬¸ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì „ë‹¬ í•  ìˆ˜ ìˆëŠ” Kinesis data streamì„ ì‚¬ìš©í•¨ìœ¼ë¡œì„œ ë°ì´í„°ì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•˜ê³  ì‹¤ì‹œê°„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘ ë° ì €ì¥í•  ìˆ˜ ìˆëŠ” ë¦¬ì†ŒìŠ¤ë¥¼ ì„ íƒí–ˆë‹¤.<br>
> Kinesis FirehoseëŠ” Kinesis Data Streamìœ¼ë¡œ ë¶€í„° ë°ì´í„°ë¥¼ ì „ë‹¬ ë°›ì•„ì„œ ì²˜ë¦¬í•˜ê³  í•„ìš”í•œ destinationì— ë³´ë‚´ì£¼ê¸° ìœ„í•´ Firehoseë¥¼ ì‚¬ìš©í•˜ì˜€ë‹¤. FirehoseëŠ” ë°ì´í„°ë¥¼ ë°›ì•„ì„œ ë³„ë‹¤ë¥¸ ì²˜ë¦¬ ê³¼ì • ì—†ì´ ë°”ë¡œ ì›í•˜ëŠ” ì„œë¹„ìŠ¤ë¡œ transfer í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì„ íƒí–ˆìŠµë‹ˆë‹¤. <br>
> ì´ ë‘ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë©”ì„¸ì§€ë¥¼ data stream ì—ì„œ firehoseë¡œ ì „ë‹¬í•  ìˆ˜ ìˆê³  firehoseì—ì„œ ì „ë‹¬ ë°›ì€ ë¡œê·¸ë“¤ì€ s3ì— ë°±ì—…í•¨ê³¼ ë™ì‹œì— ë‹¤ë¥¸ ê³¼ì •ì„ ê±°ì¹˜ì§€ ì•Šê³  destinationì¸ open search serviceë¡œ ê°ˆ ìˆ˜ ìˆë„ë¡ ë¦¬ì†ŒìŠ¤ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤.

> **AWS Open Search Service**
> <br> Open Search ServiceëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ìœ ì €ê°€ ì„¼ì„œì˜ ì •ë³´ë¥¼ ë³´ì—¬ì£¼ê³  ê·¸ ë°ì´í„°ë¥¼ ì‹œê°í™” ì‹œí‚¤ê¸° ìœ„í•œ íˆ´ë¡œ Open Serach Serviceë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ê±°ì˜ ì‹¤ì‹œê°„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì‹œê³„ì—´ ê¸°ë°˜ìœ¼ë¡œ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ì‹œê°í™” í•  ìˆ˜ ìˆê³  ì§ê´€ì ì¸ ì‹œê°í™”ë¥¼ ì œê³µí•˜ê¸° ë•Œë¬¸ì— ì´ ë¦¬ì†ŒìŠ¤ë¥¼ êµ¬ì„±í–ˆìŠµë‹ˆë‹¤. -->


## The purpose of Architecture
Hence, through this architecture can handle real time data and can be viewed by time-series and alert to farmer immediately.Using Serverless resource allow us to decouple dependency of system.

